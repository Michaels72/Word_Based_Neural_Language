{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word_Based_Neural_Language.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "PUN7PGQcam5h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cefa8a7d-f3bb-4838-bd1b-04e2a880f290"
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, LSTM, Embedding\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vunK8HgKC8tW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7K5B2KynC9Ls",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We will attempt to create a deep learning model to construct a sentence out of a slected text from Marcel Proust's book Swann's Way.  This deep learning model will take a word as input and generate a sentence as output.  Limitations include a reduced sized text due to memory constraints.  In future efforts, we will have to train on more text and increased layers.  We also have to remove the stop words from the text."
      ]
    },
    {
      "metadata": {
        "id": "rkd1NBTyVqlE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0ceb0e52-9160-4339-8fa4-9d2251613785"
      },
      "cell_type": "code",
      "source": [
        "print(\"The current directory is: \", os.getcwd())\n",
        "import os\n",
        "os.chdir(\"/content/gdrive/My Drive/Galvanize Adm/Marcel Proust\")\n",
        "print(\"The current directory is: \", os.getcwd())"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The current directory is:  /content/gdrive/My Drive/Galvanize Adm/Marcel Proust\n",
            "The current directory is:  /content/gdrive/My Drive/Galvanize Adm/Marcel Proust\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VoO--tmdVyGM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "b94662c7-a4f7-4cd1-b8f8-fe0c00b13274"
      },
      "cell_type": "code",
      "source": [
        "# Loading first 20 thousand characters from Swanns Way, not the entire text to save memory space\n",
        "txt = open('SwannsWay.txt').read()\n",
        "print(txt[:500])\n",
        "data = txt[:20000].replace('\\n', '')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "For a long time I used to go to bed early. Sometimes, when I had put out\n",
            "my candle, my eyes would close so quickly that I had not even time to\n",
            "say \"I'm going to sleep.\" And half an hour later the thought that it was\n",
            "time to go to sleep would awaken me; I would try to put away the book\n",
            "which, I imagined, was still in my hands, and to blow out the light; I\n",
            "had been thinking all the time, while I was asleep, of what I had just\n",
            "been reading, but my thoughts had run into a channel of their own,\n",
            "unt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dMbmtBVPwBpO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5f8c0db3-ff67-416d-b17e-925919bb824b"
      },
      "cell_type": "code",
      "source": [
        "len(data)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19695"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "T4zn0tclaZs2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# We are encoding the text as intergers so our model and read it\n",
        "# integer encode text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([data])\n",
        "encoded = tokenizer.texts_to_sequences([data])[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "12FypYUXa5l7",
        "colab_type": "code",
        "outputId": "ef406980-38fe-4524-a577-20272e31b67e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "len(encoded)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3474"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "cfUAhzkB4tnm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1969
        },
        "outputId": "939b40a0-e4d9-4382-8571-530084f147c3"
      },
      "cell_type": "code",
      "source": [
        "pd.DataFrame(tokenizer.word_counts, index = ['']).T.reset_index()"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>for</td>\n",
              "      <td>31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>a</td>\n",
              "      <td>69</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>long</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>time</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>i</td>\n",
              "      <td>98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>used</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>to</td>\n",
              "      <td>101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>go</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>bed</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>early</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>sometimes</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>when</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>had</td>\n",
              "      <td>43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>put</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>outmy</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>candle</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>my</td>\n",
              "      <td>78</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>eyes</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>would</td>\n",
              "      <td>41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>close</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>so</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>quickly</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>that</td>\n",
              "      <td>31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>not</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>even</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>tosay</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>i'm</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>going</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>sleep</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>and</td>\n",
              "      <td>94</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1216</th>\n",
              "      <td>arms</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1217</th>\n",
              "      <td>ofmy</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1218</th>\n",
              "      <td>mother</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1219</th>\n",
              "      <td>misfortunes</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1220</th>\n",
              "      <td>thedearer</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1221</th>\n",
              "      <td>crimes</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1222</th>\n",
              "      <td>driven</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1223</th>\n",
              "      <td>thanordinarily</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1224</th>\n",
              "      <td>scrupulous</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1225</th>\n",
              "      <td>examination</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1226</th>\n",
              "      <td>conscience</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1227</th>\n",
              "      <td>alas</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1228</th>\n",
              "      <td>stayedtalking</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1229</th>\n",
              "      <td>garden</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1230</th>\n",
              "      <td>fine</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1231</th>\n",
              "      <td>littleparlour</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1232</th>\n",
              "      <td>everyone</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1233</th>\n",
              "      <td>shelter</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1234</th>\n",
              "      <td>wet</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1235</th>\n",
              "      <td>except</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1236</th>\n",
              "      <td>mygrandmother</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1237</th>\n",
              "      <td>held</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1238</th>\n",
              "      <td>oneself</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1239</th>\n",
              "      <td>indoors</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1240</th>\n",
              "      <td>endless</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1241</th>\n",
              "      <td>discussions</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1242</th>\n",
              "      <td>father</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1243</th>\n",
              "      <td>thevery</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1244</th>\n",
              "      <td>wettest</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1245</th>\n",
              "      <td>s</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1246 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "               index     \n",
              "0                for   31\n",
              "1                  a   69\n",
              "2               long    4\n",
              "3               time   10\n",
              "4                  i   98\n",
              "5               used    5\n",
              "6                 to  101\n",
              "7                 go    5\n",
              "8                bed   13\n",
              "9              early    1\n",
              "10         sometimes    4\n",
              "11              when   16\n",
              "12               had   43\n",
              "13               put    3\n",
              "14             outmy    1\n",
              "15            candle    2\n",
              "16                my   78\n",
              "17              eyes    8\n",
              "18             would   41\n",
              "19             close    1\n",
              "20                so   10\n",
              "21           quickly    1\n",
              "22              that   31\n",
              "23               not   11\n",
              "24              even    5\n",
              "25             tosay    1\n",
              "26               i'm    1\n",
              "27             going    1\n",
              "28             sleep   13\n",
              "29               and   94\n",
              "...              ...  ...\n",
              "1216            arms    1\n",
              "1217            ofmy    1\n",
              "1218          mother    1\n",
              "1219     misfortunes    1\n",
              "1220       thedearer    1\n",
              "1221          crimes    1\n",
              "1222          driven    1\n",
              "1223  thanordinarily    1\n",
              "1224      scrupulous    1\n",
              "1225     examination    1\n",
              "1226      conscience    1\n",
              "1227            alas    1\n",
              "1228   stayedtalking    1\n",
              "1229          garden    1\n",
              "1230            fine    1\n",
              "1231   littleparlour    1\n",
              "1232        everyone    2\n",
              "1233         shelter    1\n",
              "1234             wet    1\n",
              "1235          except    1\n",
              "1236   mygrandmother    1\n",
              "1237            held    1\n",
              "1238         oneself    1\n",
              "1239         indoors    1\n",
              "1240         endless    1\n",
              "1241     discussions    1\n",
              "1242          father    1\n",
              "1243         thevery    1\n",
              "1244         wettest    1\n",
              "1245               s    1\n",
              "\n",
              "[1246 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "metadata": {
        "id": "v5C_bLRW5CLG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4zwy_l4I4dVy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "d7f65fbb-c378-494b-eab3-f4fc6fbbaad2"
      },
      "cell_type": "code",
      "source": [
        "plt.bar(pd.DataFrame(tokenizer.word_counts, index = ['']).T.reset_index()['index'], pd.DataFrame(tokenizer.word_counts, index = ['']).T.reset_index()[''])\n",
        "plt.show()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BarContainer object of 1246 artists>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfAAAAFKCAYAAADi/Q31AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAXI4oemVgtlQAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "g0b3_rChbGyL",
        "colab_type": "code",
        "outputId": "d1421c1c-1f37-401a-901b-04ca4cbdbf93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# determine the vocabulary size (how name unique words we have in the selected text)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary Size: %d' % vocab_size)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary Size: 1247\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "g-BW20f7bKO7",
        "colab_type": "code",
        "outputId": "49f6ed50-5841-45c0-dfe9-619f99b581a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "len(tokenizer.word_index)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1246"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "dMj0EBk8bwTw",
        "colab_type": "code",
        "outputId": "e4e8f307-22f3-4b2d-c6f5-8158d9ab6750",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# create word -> word sequences\n",
        "sequences = []\n",
        "for i in range(1, len(encoded)):\n",
        "\tsequence = encoded[i-1 : i+1]\n",
        "\tsequences.append(sequence)\n",
        "print('Total Sequences: %d' % len(sequences))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Sequences: 3473\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "z9Asnv-Nb1dt",
        "colab_type": "code",
        "outputId": "894c8770-9009-4be7-c5f7-c22ed213f54f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "len(sequences)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3473"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "s0-qfH9Kb8wU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# split into X and y elements\n",
        "sequences = np.array(sequences)\n",
        "X, y = sequences[:,0],sequences[:,1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kgi58Q38caUR",
        "colab_type": "code",
        "outputId": "8de766ca-315f-41da-db47-37e530cd37d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3473,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "_pkYbTeQchdb",
        "colab_type": "code",
        "outputId": "915291ee-a1b8-42b7-fea4-9c8c0dbacfa2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3473,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "Q3DE6GzXciJi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# one hot encode outputs\n",
        "y = to_categorical(y, num_classes = vocab_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0MWURXhGc34A",
        "colab_type": "code",
        "outputId": "db2bb319-764e-432e-e073-7f3c329cbd78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3473, 1247)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "INb8m4Q_c5p6",
        "colab_type": "code",
        "outputId": "390b6cef-a212-402a-fe39-4e40061a2039",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "cell_type": "code",
      "source": [
        "# define model, we are using the LSTM model with one hidden layer.  LSTM (Long Short Term Memory) to allow the machine to remember \n",
        "# sequence of words and learn the grammar\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(vocab_size, 10, input_length=1))\n",
        "model.add(LSTM(100,  return_sequences = True))\n",
        "\n",
        "model.add(LSTM(100))\n",
        "\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "print(model.summary())"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 1, 10)             12470     \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, 1, 100)            44400     \n",
            "_________________________________________________________________\n",
            "lstm_5 (LSTM)                (None, 100)               80400     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1247)              125947    \n",
            "=================================================================\n",
            "Total params: 263,217\n",
            "Trainable params: 263,217\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FHSAH5zSdOUI",
        "colab_type": "code",
        "outputId": "f77a8505-8727-4c30-fdd2-08fce2a80308",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3451
        }
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# compile network\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit network\n",
        "model.fit(X, y, epochs=100, verbose=2)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            " - 3s - loss: 6.8795 - acc: 0.0564\n",
            "Epoch 2/100\n",
            " - 1s - loss: 5.9429 - acc: 0.0593\n",
            "Epoch 3/100\n",
            " - 1s - loss: 5.7865 - acc: 0.0593\n",
            "Epoch 4/100\n",
            " - 1s - loss: 5.6873 - acc: 0.0587\n",
            "Epoch 5/100\n",
            " - 1s - loss: 5.6225 - acc: 0.0576\n",
            "Epoch 6/100\n",
            " - 1s - loss: 5.5820 - acc: 0.0553\n",
            "Epoch 7/100\n",
            " - 1s - loss: 5.5518 - acc: 0.0582\n",
            "Epoch 8/100\n",
            " - 1s - loss: 5.5203 - acc: 0.0685\n",
            "Epoch 9/100\n",
            " - 1s - loss: 5.4950 - acc: 0.0697\n",
            "Epoch 10/100\n",
            " - 1s - loss: 5.4680 - acc: 0.0760\n",
            "Epoch 11/100\n",
            " - 1s - loss: 5.4407 - acc: 0.0792\n",
            "Epoch 12/100\n",
            " - 1s - loss: 5.4176 - acc: 0.0783\n",
            "Epoch 13/100\n",
            " - 1s - loss: 5.3901 - acc: 0.0763\n",
            "Epoch 14/100\n",
            " - 1s - loss: 5.3686 - acc: 0.0783\n",
            "Epoch 15/100\n",
            " - 1s - loss: 5.3446 - acc: 0.0832\n",
            "Epoch 16/100\n",
            " - 1s - loss: 5.3230 - acc: 0.0904\n",
            "Epoch 17/100\n",
            " - 1s - loss: 5.2981 - acc: 0.0916\n",
            "Epoch 18/100\n",
            " - 1s - loss: 5.2680 - acc: 0.0959\n",
            "Epoch 19/100\n",
            " - 1s - loss: 5.2269 - acc: 0.0970\n",
            "Epoch 20/100\n",
            " - 1s - loss: 5.1806 - acc: 0.0993\n",
            "Epoch 21/100\n",
            " - 1s - loss: 5.1288 - acc: 0.1014\n",
            "Epoch 22/100\n",
            " - 1s - loss: 5.0788 - acc: 0.1008\n",
            "Epoch 23/100\n",
            " - 1s - loss: 5.0319 - acc: 0.1037\n",
            "Epoch 24/100\n",
            " - 1s - loss: 4.9869 - acc: 0.1068\n",
            "Epoch 25/100\n",
            " - 1s - loss: 4.9423 - acc: 0.1094\n",
            "Epoch 26/100\n",
            " - 1s - loss: 4.8939 - acc: 0.1054\n",
            "Epoch 27/100\n",
            " - 1s - loss: 4.8498 - acc: 0.1117\n",
            "Epoch 28/100\n",
            " - 1s - loss: 4.8052 - acc: 0.1186\n",
            "Epoch 29/100\n",
            " - 1s - loss: 4.7628 - acc: 0.1189\n",
            "Epoch 30/100\n",
            " - 1s - loss: 4.7214 - acc: 0.1333\n",
            "Epoch 31/100\n",
            " - 1s - loss: 4.6821 - acc: 0.1255\n",
            "Epoch 32/100\n",
            " - 1s - loss: 4.6412 - acc: 0.1402\n",
            "Epoch 33/100\n",
            " - 1s - loss: 4.6023 - acc: 0.1371\n",
            "Epoch 34/100\n",
            " - 1s - loss: 4.5640 - acc: 0.1402\n",
            "Epoch 35/100\n",
            " - 1s - loss: 4.5243 - acc: 0.1454\n",
            "Epoch 36/100\n",
            " - 1s - loss: 4.4839 - acc: 0.1466\n",
            "Epoch 37/100\n",
            " - 1s - loss: 4.4421 - acc: 0.1483\n",
            "Epoch 38/100\n",
            " - 1s - loss: 4.4038 - acc: 0.1601\n",
            "Epoch 39/100\n",
            " - 1s - loss: 4.3613 - acc: 0.1673\n",
            "Epoch 40/100\n",
            " - 1s - loss: 4.3178 - acc: 0.1682\n",
            "Epoch 41/100\n",
            " - 1s - loss: 4.2760 - acc: 0.1745\n",
            "Epoch 42/100\n",
            " - 1s - loss: 4.2327 - acc: 0.1754\n",
            "Epoch 43/100\n",
            " - 1s - loss: 4.1862 - acc: 0.1837\n",
            "Epoch 44/100\n",
            " - 1s - loss: 4.1413 - acc: 0.1837\n",
            "Epoch 45/100\n",
            " - 1s - loss: 4.0979 - acc: 0.1860\n",
            "Epoch 46/100\n",
            " - 1s - loss: 4.0545 - acc: 0.1903\n",
            "Epoch 47/100\n",
            " - 1s - loss: 4.0079 - acc: 0.1906\n",
            "Epoch 48/100\n",
            " - 1s - loss: 3.9690 - acc: 0.1969\n",
            "Epoch 49/100\n",
            " - 1s - loss: 3.9294 - acc: 0.2021\n",
            "Epoch 50/100\n",
            " - 1s - loss: 3.8912 - acc: 0.2018\n",
            "Epoch 51/100\n",
            " - 1s - loss: 3.8490 - acc: 0.2062\n",
            "Epoch 52/100\n",
            " - 1s - loss: 3.8153 - acc: 0.2116\n",
            "Epoch 53/100\n",
            " - 1s - loss: 3.7788 - acc: 0.2200\n",
            "Epoch 54/100\n",
            " - 1s - loss: 3.7478 - acc: 0.2243\n",
            "Epoch 55/100\n",
            " - 1s - loss: 3.7166 - acc: 0.2180\n",
            "Epoch 56/100\n",
            " - 1s - loss: 3.6876 - acc: 0.2252\n",
            "Epoch 57/100\n",
            " - 1s - loss: 3.6564 - acc: 0.2315\n",
            "Epoch 58/100\n",
            " - 1s - loss: 3.6285 - acc: 0.2335\n",
            "Epoch 59/100\n",
            " - 1s - loss: 3.5964 - acc: 0.2384\n",
            "Epoch 60/100\n",
            " - 1s - loss: 3.5741 - acc: 0.2350\n",
            "Epoch 61/100\n",
            " - 1s - loss: 3.5479 - acc: 0.2404\n",
            "Epoch 62/100\n",
            " - 1s - loss: 3.5180 - acc: 0.2459\n",
            "Epoch 63/100\n",
            " - 1s - loss: 3.4877 - acc: 0.2514\n",
            "Epoch 64/100\n",
            " - 1s - loss: 3.4656 - acc: 0.2577\n",
            "Epoch 65/100\n",
            " - 1s - loss: 3.4444 - acc: 0.2551\n",
            "Epoch 66/100\n",
            " - 1s - loss: 3.4189 - acc: 0.2580\n",
            "Epoch 67/100\n",
            " - 1s - loss: 3.3964 - acc: 0.2626\n",
            "Epoch 68/100\n",
            " - 1s - loss: 3.3741 - acc: 0.2626\n",
            "Epoch 69/100\n",
            " - 1s - loss: 3.3514 - acc: 0.2669\n",
            "Epoch 70/100\n",
            " - 1s - loss: 3.3258 - acc: 0.2620\n",
            "Epoch 71/100\n",
            " - 1s - loss: 3.3073 - acc: 0.2675\n",
            "Epoch 72/100\n",
            " - 1s - loss: 3.2869 - acc: 0.2735\n",
            "Epoch 73/100\n",
            " - 1s - loss: 3.2683 - acc: 0.2707\n",
            "Epoch 74/100\n",
            " - 1s - loss: 3.2503 - acc: 0.2796\n",
            "Epoch 75/100\n",
            " - 1s - loss: 3.2340 - acc: 0.2807\n",
            "Epoch 76/100\n",
            " - 1s - loss: 3.2163 - acc: 0.2758\n",
            "Epoch 77/100\n",
            " - 1s - loss: 3.1986 - acc: 0.2810\n",
            "Epoch 78/100\n",
            " - 1s - loss: 3.1795 - acc: 0.2825\n",
            "Epoch 79/100\n",
            " - 1s - loss: 3.1657 - acc: 0.2848\n",
            "Epoch 80/100\n",
            " - 1s - loss: 3.1478 - acc: 0.2874\n",
            "Epoch 81/100\n",
            " - 1s - loss: 3.1377 - acc: 0.2833\n",
            "Epoch 82/100\n",
            " - 1s - loss: 3.1205 - acc: 0.2862\n",
            "Epoch 83/100\n",
            " - 1s - loss: 3.1023 - acc: 0.2876\n",
            "Epoch 84/100\n",
            " - 1s - loss: 3.0870 - acc: 0.2902\n",
            "Epoch 85/100\n",
            " - 1s - loss: 3.0710 - acc: 0.2897\n",
            "Epoch 86/100\n",
            " - 1s - loss: 3.0560 - acc: 0.2931\n",
            "Epoch 87/100\n",
            " - 1s - loss: 3.0422 - acc: 0.2986\n",
            "Epoch 88/100\n",
            " - 1s - loss: 3.0284 - acc: 0.2963\n",
            "Epoch 89/100\n",
            " - 1s - loss: 3.0177 - acc: 0.2963\n",
            "Epoch 90/100\n",
            " - 1s - loss: 3.0027 - acc: 0.2983\n",
            "Epoch 91/100\n",
            " - 1s - loss: 2.9901 - acc: 0.3043\n",
            "Epoch 92/100\n",
            " - 1s - loss: 2.9827 - acc: 0.3023\n",
            "Epoch 93/100\n",
            " - 1s - loss: 2.9701 - acc: 0.3058\n",
            "Epoch 94/100\n",
            " - 1s - loss: 2.9572 - acc: 0.3069\n",
            "Epoch 95/100\n",
            " - 1s - loss: 2.9472 - acc: 0.3049\n",
            "Epoch 96/100\n",
            " - 1s - loss: 2.9375 - acc: 0.3087\n",
            "Epoch 97/100\n",
            " - 1s - loss: 2.9222 - acc: 0.3043\n",
            "Epoch 98/100\n",
            " - 1s - loss: 2.9102 - acc: 0.3092\n",
            "Epoch 99/100\n",
            " - 1s - loss: 2.8968 - acc: 0.3118\n",
            "Epoch 100/100\n",
            " - 1s - loss: 2.8888 - acc: 0.3061\n",
            "CPU times: user 2min 58s, sys: 16.1 s, total: 3min 14s\n",
            "Wall time: 2min 17s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "27U7vbm8ddIZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Make a function which takes an input (a word from the selected text, in the case below we selected 'eyes') and then use that to constuct \n",
        "# a sentence\n",
        "def generate_seq(model, tokenizer, seed_text, n_words):\n",
        "\tin_text, result = seed_text, seed_text\n",
        "\n",
        "\tfor _ in range(n_words):\n",
        "\t\t\n",
        "\t\tencoded = tokenizer.texts_to_sequences([in_text])[0]\n",
        "\t\tencoded = np.array(encoded)\n",
        "\t\t\n",
        "\t\tyhat = model.predict_classes(encoded, verbose=0)\n",
        "\t\t\n",
        "\t\tout_word = ''\n",
        "\t\tfor word, index in tokenizer.word_index.items():\n",
        "\t\t\tif index == yhat:\n",
        "\t\t\t\tout_word = word\n",
        "\t\t\t\tbreak\n",
        "\t\t\n",
        "\t\tin_text, result = out_word, result + ' ' + out_word  \n",
        "\treturn result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G4dLspkHctJ3",
        "colab_type": "code",
        "outputId": "516a362b-e250-45aa-ddd0-d1274a5e5298",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(generate_seq(model, tokenizer, 'eyes', 25))   # 25 will dictate the lenght of the sentence we want created"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "eyes and to the room in the room in the room in the room in the room in the room in the room in the room\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ipm7p5ZHcqUg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}